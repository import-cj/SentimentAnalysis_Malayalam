from sklearn_genetic import GASearchCV

from sklearn_genetic import ExponentialAdapter

from sklearn_genetic.space import Continuous, Categorical, Integer

from sklearn.ensemble import RandomForestClassifier

from sklearn.model_selection import train_test_split, StratifiedKFold

from sklearn.datasets import load_digits

from sklearn.metrics import accuracy_score

from sklearn.linear_model import LogisticRegression

from sklearn.svm import SVC

from sklearn.utils import class_weight

import numpy as np

from xgboost import XGBClassifier



def testtrainsplit(df,frac=0.20):

  from sklearn.model_selection import train_test_split

  trainx = [] 

  testx = []

  for i in [0,1,2]:

     tdf = df[df['sentiment'] == i]

     t1, t2 = train_test_split(tdf, test_size=frac, random_state=0)

     num=len(tdf)

     trainx.append(t1)

     testx.append(t2)

  return pd.concat(trainx),pd.concat(testx)



def getModelTuner(dataframe):

  traindf, testdf = testtrainsplit(dataframe)

  X_train = traindf.iloc[:, :-1]

  X_test = testdf.iloc[:, :-1]

  y_train = traindf.sentiment

  y_test = testdf.sentiment



  class_weights = class_weight.compute_class_weight(class_weight = 'balanced',

                                                  classes = np.unique(y_train),

                                                  y=y_train)

  class_weights = dict(zip(np.unique(y_train), class_weights))

  classifiers = {

      "SVC": SVC(probability=True,class_weight=class_weights),

      "LR": LogisticRegression(multi_class='multinomial',max_iter=1001,class_weight=class_weights),

      "RF": RandomForestClassifier(class_weight=class_weights),

      "XGB": XGBClassifier(n_estimators=500,learning_rate=1, max_depth=2,objective='multi:softmax',num_class=3,tree_method='gpu_hist'),

  }



  params_grid = {

          "SVC": {

              "C": Continuous(0,1000),

              "gamma": Continuous(0,10),

              "kernel": Categorical(["linear", "rbf"])

          },

          "LR": {

              "C": Continuous(0,10),

              "penalty": Categorical(['none', 'l2']),

              "solver": Categorical(['lbfgs', 'saga'])

          },

          "RF" : {

              'min_weight_fraction_leaf': Continuous(0.01, 0.5, distribution='log-uniform'),

              'bootstrap': Categorical([True, False]),

              'criterion': Categorical(['gini', 'entropy']),

              'max_depth': Integer(2, 30),

              'min_samples_split': Continuous(0, 1),

              'min_samples_leaf': Integer(1, 5),

              'max_leaf_nodes': Integer(2, 50),

              'n_estimators': Integer(10, 500),

              'max_features': Categorical(['auto', 'sqrt', 'log2']),

          },

          "XGB" : {

              'max_depth': Integer(2, 30),

              'learning_rate': Continuous(0.3,1.7,distribution='log-uniform'),

              'gamma':Continuous(0,3,distribution='uniform'),

              'reg_lambda':Continuous(0,10,distribution='uniform'),

              'scale_pos_weight': Continuous(0,2),

              'n_estimators':Integer(200,800),

              'min_child_weight':Continuous(0,10),

              'subsample':Continuous(0.5,1),

              'colsample_bytree':Continuous(0.5,1),

              'colsample_bylevel':Continuous(0.5,1),

              'reg_alpha':Continuous(0,20,distribution='uniform'),

          }

  }

  def evolved_classifier(classifier_name, njobs=-1):

      nonlocal X_train

      nonlocal X_test

      nonlocal y_test

      nonlocal y_train

      classifier = classifiers[classifier_name]

      cv = StratifiedKFold(n_splits=3, shuffle=True)

      params = params_grid[classifier_name] 

      mutation_adapter = ExponentialAdapter(initial_value=0.8, end_value=0.2, adaptive_rate=0.1)

      crossover_adapter = ExponentialAdapter(initial_value=0.2, end_value=0.8, adaptive_rate=0.1)

      evolved_estimator = GASearchCV(

          estimator=classifier,

          cv=cv,

          scoring="accuracy",

          population_size=15,

          generations=30,

          crossover_probability=crossover_adapter,

          mutation_probability=mutation_adapter,

          param_grid=params,

          algorithm="eaSimple",

          n_jobs=njobs,

          #verbose=True

      )    

      evolved_estimator.fit(X_train, y_train)

      # Best parameters found

      print(evolved_estimator.best_params_)

      # Use the model fitted with the best parameters

      y_predict_ga = evolved_estimator.predict(X_test)

      print(classification_report(y_test, y_predict_ga, labels=[0,1,2]))

      return evolved_estimator.best_params_

      # Saved metadata for further analysis

      #print("Stats achieved in each generation: ", evolved_estimator.history)

      #print("Best k solutions: ", evolved_estimator.hof)

      #return evolved_estimator

  return evolved_classifie

!pip install sklearn-genetic-opt
# pass encoded df not text directly. last column sentiment. rest all columns encoded text as numerical features
tuner = getModelTuner(df)
model_names = ['XGB', 'RF']#, 'LR', 'SVC']
for mname in model_names:
	p = tuner(mname)
	print(p)

